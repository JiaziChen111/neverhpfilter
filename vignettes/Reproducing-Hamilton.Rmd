---
title: "Reproducing Hamilton"
author: "Justin M Shea"
date: ' '
output:
  rmarkdown::html_document:
    toc: yes
vignette: >
  %\VignetteIndexEntry{Reproducing Hamilton}  
  %\VignetteEngine{knitr::rmarkdown}  
  %\VignetteEncoding{UTF-8}
---


\newpage

  In the working paper titled "Why You Should Never Use the **H**odrick-**P**rescott Filter", James D. Hamilton proposes an approach to economic time series filtering which achieves goals the HP-Filter attempts, but ultimately does not produce. 

The abstract offers an excellent introduction.


  > 
  (1) The HP filter produces series with spurious dynamic relations that have no basis in the underlying data-generating process.  
  (2) Filtered values at the end of the sample are very different from those in the middle, and are also characterized by spurious dynamics.  
  (3) A statistical formalization of the problem typically produces values for the smoothing parameter vastly at odds with common practice, e.g., a value for $\lambda$ far below 1600 for quarterly data.  
  (4) There's a better alternative. A regression of the variable at date $t + h$ on the four most recent values as of date $t$ offers a robust approach to detrending that achieves all the objectives sought by users of the HP filter with none of its drawbacks.

The **`neverhpfilter`** package provides functions for implementing his solution.
[Hamilton (2017) <doi:10.3386/w23429>](https://www.nber.org/papers/w23429)

## A Better Alternative

Using an example of quarterly economic data, Hamilton suggests a linear model dependent on an `h = 8` look-ahead period, which is independent of `p = 4` lagged variables. An auto-regressive $AR(p)$ model, with a $t+h$ look-ahead twist, if you will. This can be expressed more specifically by:

$$y_{t+8} = \beta_0 + \beta_1 y_t + \beta_2 y_{t-1} +\beta_3 y_{t-2} + \beta_4 y_{t-3} + v_{t+8}$$
$$\hat{v}_{t+8} = y_{t+8} + \hat{\beta}_0 + \hat{\beta}_1 y_t + \hat{\beta}_2 y_{t-1} + \hat{\beta}_3 y_{t-2} + \hat{\beta}_4 y_{t-3}$$

Which can be rewritten as:

$$y_{t} = \beta_0 + \beta_1 y_{t-8} + \beta_2 y_{t-9} + \beta_3 y_{t-10} + \beta_4 y_{t-11} + v_{t}$$

$$\hat{v}_{t} = y_{t} - \hat{\beta}_0 + \hat{\beta}_1 y_{t-8} + \hat{\beta}_2 y_{t-9} + \hat{\beta}_3 y_{t-10} + \hat{\beta}_4 y_{t-11}$$



## Implementation

```{r, message = FALSE, warning = FALSE}
library(xts)
library(knitr)
library(neverhpfilter)
```

In this next section, I reproduce a few of Hamilton's tables and graphs, to make
sure the functions approximately match his results.

In the Appendix, Employment is plotted in the form of $100 * log($`PAYEMS`$)$, 
or the All Employees: Total Non-farm series (Hamilton 44).

```{r, warning = FALSE, message = FALSE}
data(PAYEMS)
PAYEMS_qtr <- xts::to.quarterly(PAYEMS["1947/"], OHLC = FALSE)
log_Employment <- 100*log(PAYEMS_qtr)

employ_trend <- yth_filter(log_Employment, h = 8, p = 4, output = c("x", "trend"), family = gaussian)

plot(employ_trend, grid.col = "white", legend.loc = "topleft", main = "Log of Employment and trend")
```

The cycle component is of great interest. Here, it is graphed alongside
a random walk representation, defined as the difference between the current observation $t$,
and the look-ahead parameter, $h$ (Hamilton 44).

```{r, warning = FALSE}
employ_cycle <- yth_filter(log_Employment, h = 8, p = 4, output = c("cycle", "random"), family = gaussian)

plot(employ_cycle, grid.col = "white", legend.loc = "topright", main="Log of Employment cycle and random walk")
abline(h=0)
```

Turning the page, we find a similar graph of the cyclical component of $100 * log$ of GDP, Exports, Consumption, Imports, Investment, and Government superimposed with their random walk representations. (Hamilton 45). 

Below I `merge` these data into one `xts` object and write a function wrapper around `yth_filter` and `plot`, which is then `lapply`'d over each series, producing a plot for each one.

```{r, message=FALSE, warning=FALSE}
fig6_data <- 100*log(merge(GDPC1, EXPGSC1, PCECC96, IMPGSC1, GDPIC1, GCEC1)["1947/2016-3"])

fig6_wrapper <- function(x, ...) {
  
               cycle <-  yth_filter(x, h = 8, p = 4, output = c("cycle", "random"), family = gaussian)
               plot(cycle, grid.col = "white", lwd=1, main = paste0(names(cycle)[1]," and random"))
}
```

```{r, warning=FALSE, message=FALSE, eval=FALSE}
par(mfrow=c(3,2))
lapply(fig6_data, fig6_wrapper)
```

```{r, echo=FALSE,results='hide',fig.keep='all'}
par(mfrow=c(3,2))
lapply(fig6_data, fig6_wrapper)
```

# Comparing our estimates with Hamilton's

When striving to recreate a statistical method found in a journal or paper, one can perform surprisingly well by thoroughly digesting the relevant sections and "eyeballing" graphs included in the authors work. 

Better still, is a table presenting the authors results, which one may use to direclty
compare with their own reproduction. Fortunately for us, Hamilton's Appendix displays such a table which I use to test against estimates computed with functions contained in **`neverhpfilter`**.

His results are displayed below in table 2 (Hamilton 40), which I've stored as a `data.frame` in this package.

```{r, eval=FALSE}
data("Hamilton_table_2")
?Hamilton_table_2
```

```{r}
kable(Hamilton_table_2[-NROW(Hamilton_table_2),], align = 'l', caption = "Hamilton's results: table 2, pg. 40")
```


I'll replicate the table above, combining base R functions with estimates of the `yth_filter` function.

Per the usual protocol when approaching such a problem, the first step is to combine
data in manner that allows for convenient iteration of computations across all data sets.
First, I `merge` series which already have a quarterly frequency. These are `GDPC1, PCECC96, GDPIC1, EXPGSC1, IMPGSC1, GCEC1, GDPDEF`. At this step, we can also subset observations by the date range used by Hamilton. As all series of which units are measured in prices need to be given the $100*log$ treatment, I add that to this step as well.

```{r, warning = FALSE, message = FALSE}
quarterly_data <- 100*log(merge(GDPC1, PCECC96, GDPIC1, EXPGSC1, IMPGSC1, GCEC1, GDPDEF)["1947/2016-3"])
```

Some of the series we wish to compare have a monthly periodicity, so we need to lower their frequency `to.quarterly`. First, `merge` monthly series and $100*log$ those expressed in prices, not percentages. Then, functionally iterate over every series and transform them `to.quarterly`.
Presumably because more data was available at the time of Hamilton's work, monthly series include observations from the second quarter of 2016 and so I subset accordingly. Finally, all series are combined into one `xts` object, `quarterly_data`.

```{r, warning = FALSE, message = FALSE}
monthly_data <- merge(100*log(PAYEMS), 100*log(SP500$SP500)["1950/"], UNRATENSA, GS10, FEDFUNDS)

to_quarterly_data <- do.call(merge, lapply(monthly_data, to.quarterly, OHLC = FALSE))["1947/2016-6"]

quarterly_data <- merge(quarterly_data, to_quarterly_data)
```

Now that the data has been prepped, its time to functionally iterate over each series, `lapply`ing the `yth_filter` to all. The optional argument of `output = "cycle"` comes in handy because it returns the labeled univariate cycle component for each series. The same can be done for the `random` component as well.

```{r, warning = FALSE, message = FALSE}
cycle <- do.call(merge, lapply(quarterly_data, yth_filter, output = "cycle"))

random <- do.call(merge, lapply(quarterly_data, yth_filter, output = "random"))
```

Now that all data have been transformed into both cycle and random components, its 
time to estimate the standard deviation for each, as well as each components correlation 
with GDP. This is also a good opportunity to `t`ranspose each of our estimates into vertical columned `data.frames`, matching Hamilton's format. 

```{r, warning = FALSE, message = FALSE}
cycle.sd <- t(data.frame(lapply(cycle, sd, na.rm = TRUE)))
GDP.cor <- t(data.frame(lapply(cycle, cor, cycle[,1], use = "complete.obs")))
random.sd <- t(data.frame(lapply(random, sd, na.rm = TRUE)))
random.cor <- t(data.frame(lapply(random, cor, random[,1], use = "complete.obs")))

my_table_2 <- round(data.frame(cbind(cycle.sd, GDP.cor, random.sd, random.cor)), 2)
```

Hamilton displays the date ranges of his samples so we will do the same. 

I use a simple function I call `sample_range` to extract the first and last observation of each
series' `index.xts`. This approach serves as a check on the work, as oppose to 
manually creating labels.

Sample ranges are then `t`ransposed into vertical `data.frames` and `cbind`'d to 
the existing table of estimates. 

```{r, warning = FALSE, message = FALSE}
sample_range <- function(x) {
  x <- na.omit(x)
  gsub(" ", "-", paste0(index(x[1,]), "/", index(x[NROW(x),])))
}

data_sample <- t(data.frame(lapply(quarterly_data, sample_range)))

my_table_2 <- cbind(my_table_2, data_sample)
names(my_table_2) <- names(Hamilton_table_2)
```


Finally, `rbind` Hamilton's table 2 with my table and compare. They are nearly 
identical, inspiring confidence in the repilcation of this approach.


```{r, warning = FALSE, message = FALSE, caption = "Comparison table: Hamilton vs neverhpfilter"}
 # Combined table
combined_table <- rbind(Hamilton_table_2[-NROW(Hamilton_table_2),], my_table_2)
combined_table <- combined_table[order(combined_table$cycle.sd),]
kable(combined_table, align = 'l', caption = "Hamilton's table 2 compared with estimates from neverhpfilter::yth_filter, sorted by sd. of the cycle component.")
```

# Summary

The estimates generated with the `neverhpfilter` package are nearly identical to those displayed by Hamilton(2017). If one has the use case, the generalized functions will estimate higher frequency time series as well as error distributions other than Gaussian. In addition to consulting the paper which inspired this package, check out the documentation for
`yth_filter` to learn more.

